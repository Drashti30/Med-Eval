# MedEval: G-Eval for Clinical Summarization

This repository contains the implementation of **MedEval**, a novel evaluation framework for assessing discharge summaries generated by large language models (LLMs) in clinical contexts. MedEval leverages **G-Eval**, a GPT-4-based evaluation method, and applies it using **Chain-of-Thought (CoT) prompting** to measure clinical accuracy, completeness, readability, and actionability.

## ğŸ” Project Summary

Traditional evaluation metrics like ROUGE and BLEU fall short when applied to clinical text due to their focus on lexical overlap. **MedEval** addresses this gap by using a large language modelâ€“based evaluation framework that aligns closely with clinician expectations and medical relevance.

We evaluate summaries generated by **LLaMA-2-7B** and **Mistral-7B** models, using real ICU patient data from the **MIMIC-III** dataset.


## ğŸ§  Models Used

- **LLaMA-2-7B** (via Ollama)
- **Mistral-7B** (via Hugging Face)

These models were selected for their ability to handle structured prompts and generate coherent discharge summaries in a zero-shot setting.

## ğŸ“Š Evaluation Metrics (MedEval)

Each model-generated summary was scored on:
- **Clinical Accuracy**: Alignment with true diagnoses and procedures
- **Completeness**: Inclusion of all relevant medical details
- **Readability & Fluency**: Clarity, grammar, and narrative flow
- **Actionability**: Effectiveness of follow-up instructions

Scores were provided using a CoT-based rubric (0â€“10) per dimension.

## âš™ï¸ System Architecture

MedEval uses a 3-module pipeline:
1. **Data Pipeline**: Preprocessing MIMIC-III dataset, extracting key clinical metadata
2. **Model Pipeline**: Prompting LLMs with structured inputs
3. **Evaluation Loop**: G-Evalâ€“based scoring using GPT-style rubric prompts

## ğŸ“ˆ Results Summary

- **Mistral-7B** performed better in **readability** and **actionability**
- **LLaMA2-7B** scored higher in **clinical accuracy** and **completeness**
- MedEval provided a more clinically meaningful assessment compared to BLEU/ROUGE

## ğŸ“š Dataset

- [MIMIC-III](https://www.kaggle.com/datasets/asjad99/mimiciii): Public ICU EHR dataset used for training and evaluation

## ğŸ§ª Future Work

- Incorporate **probabilistic scoring**
- Evaluate with clinically fine-tuned LLMs like **BioGPT**, **ClinicalT5**, and **MedAlpaca**
- Expand to include larger datasets and external evaluation partners

## ğŸ“„ License

This project is for academic use only. Refer to the respective licenses of the datasets and models used.

## ğŸ‘©â€ğŸ’» Authors

- Drashti Miteshkumar Mehta  
- Harini Varanasi  
- Akhila Madanapati  
- Anjali Bheemireddy

> ğŸ“ Project submitted for **DTSC 5082 - Research Methodology**, University of North Texas.

---
